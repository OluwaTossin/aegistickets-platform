---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: aegistickets-slis
  namespace: monitoring
  labels:
    app: kube-prometheus-stack
    release: kube-prometheus-stack
spec:
  groups:
  - name: sli-recording-rules
    interval: 30s
    rules:
    # Total request rate (per job)
    - record: job:http_requests_total:rate5m
      expr: sum by (job) (rate(http_requests_total[5m]))

    # 5xx error rate (per job)
    - record: job:http_requests_5xx:rate5m
      expr: sum by (job) (rate(http_requests_total{code=~"5.."}[5m]))

    # Availability ratio (success rate)
    - record: job:availability_ratio:5m
      expr: |
        1 - (
          job:http_requests_5xx:rate5m
          /
          job:http_requests_total:rate5m
        )

    # p95 latency
    - record: job:latency_p95:5m
      expr: |
        histogram_quantile(0.95,
          sum by (le, job) (rate(http_request_latency_seconds_bucket[5m]))
        )

    # p99 latency
    - record: job:latency_p99:5m
      expr: |
        histogram_quantile(0.99,
          sum by (le, job) (rate(http_request_latency_seconds_bucket[5m]))
        )

    # Request rate per endpoint
    - record: endpoint:http_requests_total:rate1m
      expr: sum by (endpoint, method) (rate(http_requests_total[1m]))

    # Error rate per endpoint
    - record: endpoint:http_errors_total:rate1m
      expr: sum by (endpoint, method) (rate(http_errors_total[1m]))

  - name: slo-alerts
    interval: 30s
    rules:
    # Fast burn: Availability SLO breach (critical)
    - alert: SLOAvailabilityFastBurn
      expr: (1 - job:availability_ratio:5m) > 0.01
      for: 10m
      labels:
        severity: critical
        slo: availability
        burn_rate: fast
      annotations:
        summary: "Fast burn detected on availability SLO"
        description: |
          The 5xx error rate is {{ $value | humanizePercentage }} over the last 5 minutes.
          This exceeds the 99.9% availability SLO.
          Error budget is burning rapidly.
          
          Current availability: {{ with query "job:availability_ratio:5m" }}{{ . | first | value | humanizePercentage }}{{ end }}
          
          Recommended actions:
          1. Check recent deployments
          2. Review backend logs for 5xx errors
          3. Inspect database connectivity
          4. Consider rollback if deploy-correlated

    # Slow burn: Sustained availability degradation (major)
    - alert: SLOAvailabilitySlowBurn
      expr: (1 - job:availability_ratio:5m) > 0.001
      for: 1h
      labels:
        severity: major
        slo: availability
        burn_rate: slow
      annotations:
        summary: "Slow burn on availability SLO detected"
        description: |
          Sustained error rate above acceptable levels for 1 hour.
          Current error rate: {{ $value | humanizePercentage }}
          
          This consumes error budget slowly but steadily.

    # Latency p95 breach (high severity)
    - alert: LatencyP95Breaching
      expr: job:latency_p95:5m > 0.8
      for: 15m
      labels:
        severity: high
        slo: latency
      annotations:
        summary: "Latency p95 exceeds 800ms SLO"
        description: |
          The p95 latency is {{ $value | humanizeDuration }} for the last 15 minutes.
          SLO target: 800ms (0.8s)
          
          Recommended actions:
          1. Check HPA events and pod CPU throttling
          2. Review database connection pool saturation
          3. Analyze slow query logs
          4. Consider scaling backend replicas

    # Database connection saturation (warning)
    - alert: DatabaseConnectionSaturation
      expr: |
        (db_active_connections / 100) > 0.8
      for: 5m
      labels:
        severity: warning
        slo: saturation
      annotations:
        summary: "Database connection pool near saturation"
        description: |
          Database connections are at {{ $value | humanizePercentage }} of max capacity.
          SLO: ≤80% for ≥99% of 5-min windows.
          
          Recommended actions:
          1. Scale backend pod replicas (increases pool size)
          2. Review long-running queries
          3. Consider upgrading RDS instance class

    # High request error rate (critical)
    - alert: HighErrorRate
      expr: |
        sum(rate(http_errors_total[5m])) by (job)
        /
        sum(rate(http_requests_total[5m])) by (job)
        > 0.05
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "High error rate detected"
        description: |
          Error rate is {{ $value | humanizePercentage }} for job {{ $labels.job }}.
          Threshold: 5%

---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: aegistickets-infrastructure
  namespace: monitoring
  labels:
    app: kube-prometheus-stack
    release: kube-prometheus-stack
spec:
  groups:
  - name: infrastructure-alerts
    interval: 30s
    rules:
    # Pod crash loop
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total{namespace="tickets-dev"}[15m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.pod }} is crash looping"
        description: |
          Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is crash looping.

    # High pod memory usage
    - alert: PodMemoryUsageHigh
      expr: |
        (container_memory_working_set_bytes{namespace="tickets-dev"}
        / container_spec_memory_limit_bytes{namespace="tickets-dev"}) > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.pod }} memory usage is high"
        description: |
          Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of its memory limit.

    # High pod CPU usage
    - alert: PodCPUUsageHigh
      expr: |
        rate(container_cpu_usage_seconds_total{namespace="tickets-dev"}[5m])
        / container_spec_cpu_quota{namespace="tickets-dev"} * 100000 > 90
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.pod }} CPU usage is high"
        description: |
          Pod {{ $labels.pod }} is using high CPU for 15 minutes.
